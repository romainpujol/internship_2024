\documentclass{amsart}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{longtable}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{titletoc}
\usepackage{appendix}
\usepackage{bbold}
\usepackage{enumitem}
\usepackage{amssymb}
\usepackage[boxed, linesnumbered]{algorithm2e}
\usepackage[left=2cm,right=2cm,top=3cm,bottom=3cm]{geometry}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{xcolor}
\usepackage{lastpage} 
\usepackage{setspace}
\usepackage{titlesec}
\usepackage{dsfont}
\usepackage{cleveref}
\usepackage{bm}
\usepackage{extramarks}
\usepackage[colorinlistoftodos,bordercolor=orange,backgroundcolor=orange!20,linecolor=orange,textsize=scriptsize]{todonotes}
\usepackage{subcaption}
\SetAlgoNlRelativeSize{-1}

\geometry{
    top=1in, % Marge supérieure
    bottom=1.5in, % Marge inférieure
    left=1in, % Marge gauche
    right=1in, % Marge droite
    footskip=1in % Espace entre le bas du texte principal et le pied de page
}


\pagestyle{fancy}
\fancyhf{} 

\fancypagestyle{plain}{
    \fancyhf{} 
}
\fancyhead[C]{\normalfont\leftmark}

\titleformat{\section}[hang]
  {\normalfont\Large\bfseries} % Format du texte
  {\thesection{} - } % Préfixe (ex. 1.1 - )
  {0pt} % Espace entre le préfixe et le titre
  {\Large\bfseries} % Format du titre

% Configurer les sous-sections
\titleformat{\subsection}[hang]
  {\normalfont\normalsize\bfseries} % Format du texte
  {\thesubsection{} - } % Préfixe (ex. 1.1.1 - )
  {0pt} % Espace entre le préfixe et le titre
  {\normalsize\bfseries} % Format du titre

% Configurer les sous-sous-sections
\titleformat{\subsubsection}[hang]
  {\normalfont\small\bfseries} % Format du texte
  {\thesubsubsection{} - } % Préfixe (ex. 1.1.1.1 - )
  {0pt} % Espace entre le préfixe et le titre
  {\small\bfseries} % Format du titre
  
\ifdefined\theorem\else \newtheorem{theorem}{Theorem}\fi
\ifdefined\proposition\else \newtheorem{proposition}[theorem]{Proposition}\fi
\ifdefined\definition\else \newtheorem{definition}[theorem]{Definition}\fi
\ifdefined\lemma\else \newtheorem{lemma}[theorem]{Lemma}\fi
\ifdefined\corollary\else \newtheorem{corollary}[theorem]{Corollary}\fi
\ifdefined\remark\else \newtheorem{remark}[theorem]{Remark}\fi
\ifdefined\assumption\else \newtheorem{assumption}{Assumption}\fi
\ifdefined\example\else \newtheorem{example}{Example}\fi
\onehalfspacing


\newcommand{\argmin}{\mathop{\arg\min}}
\crefformat{enumi}{#2#1#3}
\newcommand{\nb}[3]{
		{\colorbox{#2}{\bfseries\sffamily\tiny\textcolor{white}{#1}}}
		{\textcolor{#2}{\text{$\blacktriangleright$}{\textcolor{#2}{#3}}\text{$\blacktriangleleft$}}}}
\newcommand{\rp}[1]{\nb{RP}{red}{#1}}
\newcommand{\af}[1]{\nb{AF}{blue}{#1}}
\newcommand{\bt}[1]{\nb{BT}{orange}{#1}}
\newcommand{\RR}{\mathbb{R}}

\renewcommand{\contentsname}{Table of contents}
\renewcommand{\sectionmark}[1]{\markboth{\thesection. #1}{}}

\begin{document}

\thispagestyle{empty}

\begin{figure}[htbp]
    \centering
    \begin{minipage}[b]{0.22\textwidth}
        \centering
        \includegraphics[width=\textwidth]{logo/logo.jpg}
    \end{minipage}
    \hfill
    \begin{minipage}[b]{0.25\textwidth}
        \centering
        \includegraphics[width=\textwidth]{logo/universita di pisa.png}
    \end{minipage}
\end{figure}

\vspace{1.5cm}	

\begin{center}	
    {\huge \bf\ Research Internship  \\
    \vspace{0.8cm}
    \huge Scenario Reduction Techniques for Two-Stage Stochastic Optimization \\
    \vspace{1.25cm}

    \normalsize Field of study: Applied Mathematics \\
    School year: 2023 - 2024 \\



    \vspace{3.5cm}
    \textcolor{red}{Confidentiality notice:}\\
    \color{black}
    \vspace{0.5cm}
    }
\end{center}

\noindent\normalsize \textbf{Author} Romain Pujol  \\
\textbf{Class} 2025 \\

\noindent\textbf{ENSTA Paris supervisor} Sorin-Mihai Grad \\ 
\textbf{Università di Pisa supervisor} Antonio Frangioni \\

\noindent Internship carried out from 13/05/2024 to 02/08/2024. 
\\
Università di Pisa, Largo B. Pontecorvo, 3.


\newpage
\thispagestyle{empty}
\mbox{}
\newpage



%%% PAGE DE CONFIDENTIALITE
%%% PAGE DE CONFIDENTIALITE
%%% PAGE DE CONFIDENTIALITE
%%% PAGE DE CONFIDENTIALITE
%%% PAGE DE CONFIDENTIALITE
%%% PAGE DE CONFIDENTIALITE
%%% PAGE DE CONFIDENTIALITE



\begin{titlepage}

    \vspace*{5cm}
        \Large\textbf{Acknowledgements}
    \vspace{1cm}
    
    \normalsize Thanks.
\end{titlepage}

\newpage

\begin{titlepage}

    \vspace*{1cm}
        \Large\textbf{Abstract} \\
    
    \normalsize Two-stage stochastic optimization problems become extremely large when the distributions involved have a large number of atoms. To address this issue, we can \emph{reduce} the size of the distributions to decrease the number of variables and constraints in the problem, hence facilitating the resolution of the optimization problem. It is required to ensure that the reduced distribution maintains the structure and characteristics of the original distribution. The proximity between distributions is quantified using Wasserstein distances. In order to integrate scenario reduction techniques into the SMS++ ecosystem of the University of Pisa, attention is paid to algorithmic efficiency. \\

    
    \textbf{Key-words:} Stochastic programming, scenario reduction, Wasserstein distance, greedy algorithm, local-search, stability.
  \\

        \vspace*{1cm}
        \Large\textbf{Résumé} \\
    
    \normalsize Les problèmes d'optimisation stochastique à deux étapes sont de très grande taille lorsque les distributions mises en jeu ont un grand nombre d'atomes. Pour combattre cet écueil, on peut tenter de \emph{réduire} la taille des distributions pour diminuer le nombre de variables et de contraintes du problème et donc faciliter la résolution du problème d'optimisation. Il faut s'assurer que la distribution réduite garde la structure et les caractéristiques de la distribution initiale. On quantifie la proximité entre distributions grâce aux distances de Wasserstein. Dans le but d'intégrer les techniques de réduction de scénario dans l'écosystème SMS++ de l'Université de Pise, une attention particulière est portée à l'efficacité algorithmique.  \\
    
    \textbf{Mots-clés :} Optimisation stochastique, réduction de scénario, distance de Wasserstein, algorithme glouton, recherche locale, stabilité.
    
\end{titlepage}

\tableofcontents

\newpage
\pagestyle{fancy} % Activer le style fancy
\fancyfoot[R]{\thepage/\pageref{LastPage}} 
\fancyfoot[C]{Romain Pujol / Università di Pisa \\ \textcolor{red}{Confidential / Non confidential report}}
\section{$\quad$ Introduction}
\section{$\quad$ Stability in stochastic programming}\label{stability}
\subsection{Context and notations}
Many Stochastic Programming (SP) models can be written as:
\begin{equation}\label{stochastic}
\min\left\{\int_\Xi F_0\left(x,\xi\right)\text{d}P\left(\xi\right)\:,\: x\in X, \: \int_\Xi F_j\left(x,\xi\right)\text{d}P\left(\xi\right)\leq0, j=1,...,d\right\}.
\end{equation}

Here $X\subset\RR^m$ is closed, $\Xi\subset\RR^s$ is closed, $F_j$ are random lower semi-continuous functions for all $j\in\{0,...,d\}$ and $P\in\mathcal{P}\left(\Xi\right)$ a Borel probability measure on $\Xi$. For this work, we will mostly work with discrete distributions. A \emph{discrete distribution} $P$ is written as $P=\sum_{i\in I}\delta_{x_i}p_i$ with the atoms $x_i\in\Xi$, the probabilities $p_i$ are positive and sum up to 1, the set $I$ is at most countable and $\delta$ is a Dirac measure. The set $X$ does not depend on $P$ and defines constraints on $x$. The goal of this part is to highlight a specific metric on distributions that would ensure that if two distributions $P$ and $Q$ are close, then the value of both SP models are close. We write:
\begin{align*}
    &\mathcal{X}\left(P\right)=\left\{x\in X, \int_\Xi F_j\left(x,\xi\right)\text{d}P\left(\xi\right)\leq0, j=1,...,d\right\},\\
    &v\left(P\right)=\inf\left\{\int_\Xi F_0\left(x,\xi\right)\text{d}P\left(\xi\right)\:,\: x\in \mathcal{X}\left(P\right)\right\}, \\
    &X^*\left(P\right)=\argmin\left\{\int_\Xi F_0\left(x,\xi\right)\text{d}P\left(\xi\right)\:,\: x\in X, \: \int_\Xi F_j\left(x,\xi\right)\text{d}P\left(\xi\right)\leq0, j=1,...,d\right\}.
\end{align*}
So $\mathcal{X}\left(P\right)$ denotes the feasible set, $v\left(P\right)$ the optimal value of Problem \eqref{stochastic} and $X^*\left(P\right)$ its set of minimizers. For every integer $p\geq 1$, let us define $\mathcal{F}_p\left(\Xi\right)$ a class of measurable functions from $\Xi$ to $\RR$ named locally Lipschitz continuous functions:
\begin{align*}
    &\mathcal{F}_p\left(\Xi\right)=\left\{f:\Xi\to \RR: \lvert f\left(\xi\right)-f\left(\xi'\right)\rvert \leq c_p\left(\xi,\xi'\right)\lVert\xi-\xi'\rVert, \forall \xi,\xi'\in \Xi^2 \right\}, \\
   & c_p\left(\xi,\xi'\right)=\max \left\{1,\lVert\xi \rVert,\lVert\xi'\rVert\right\}^{p-1}.
\end{align*}
When $p=1$, we get 1-Lipschitz functions. When $p\geq 2$, there exists another dependency on the space $\Xi$ allowing functions with higher variations as $p$ increases. One can note that $F_1\left(\Xi\right)\subset F_2\left(\Xi\right)\subset F_3\left(\Xi\right) \hdots$ For $P,Q\in\mathcal{P}_p\left(\Xi\right)^2$ where $\mathcal{P}_p\left(\Xi\right)=\left\{Q\in\mathcal{P}\left(\Xi\right), \int_\Xi \lVert\xi\rVert^pdQ\left(\xi\right)\right\}$, the latest statement allows us to define $p$-th order Fortet-Mourier metric as:
$$
\zeta_p\left(P,Q\right)=\sup_{f\in\mathcal{F}_p\left(\Xi\right)}\lvert \int_\Xi f\left(\xi\right)\text{d}P\left(\xi\right)-\int_\Xi f\left(\xi\right)\text{d}Q\left(\xi\right)\rvert.
$$
See \ref{appendix metric}\href{https://www.imo.universite-paris-saclay.fr/~pierre-loic.meliot/master/exam-2017.pdf}{metric proof}.

\subsection{Result}
Let $Q\in\mathcal{P}\left(\Xi\right)$, we write the perturbed model:
\begin{equation*}
    \min\left\{\int_\Xi F_0\left(x,\xi\right)\text{d}Q\left(\xi\right)\:,\: x\in X, \: \int_\Xi F_j\left(x,\xi\right)\text{d}Q\left(\xi\right)\leq0, j=1,...,d\right\},
\end{equation*}
which is Problem \eqref{stochastic} but with the distribution $Q$ instead of $P$. In this part we will try to compare the notions we have presented in the above part, for instance, we want to bound $\lvert v\left(P\right)-v\left(Q\right)\rvert$ in terms of distribution metrics. \cite[Corollary 14]{romisch_stability_2003} gives:
\begin{theorem}\label{stability_th}
    Let the number of constraints in Problem \eqref{stochastic} be $d=0$ and assume that:
    \begin{enumerate}
        \item The  solution set $X^*\left(P\right)$ is nonempty and $\mathcal{U}$ is an open, bounded neighbourhood of $X^*\left(P\right)$.
        \item The set $X$ is convex and $F_0\left(\cdot,\xi\right)$ is convex on $\RR^m$ for each $\xi\in\Xi$.
        \item there exist constants $L>0, p\geq1$ such that $\frac{1}{L}F_0\left(x,\cdot \right)\in\mathcal{F}_p\left(\Xi\right)$ for each $x\in X\cap cl\mathcal{U}$. 
    \end{enumerate}
    Then there exists a constant $\delta>0$ such that:
    \begin{align*}
        &\lvert v\left(P\right)-v\left(Q\right)\rvert \leq L\zeta_p\left(P,Q\right) \\
        & \emptyset \ne X^*\left(Q\right)\subset X^*\left(P\right)+\Psi_P\left(L\zeta_p\left(P,Q\right)\right)\mathbb{B}
    \end{align*}
    Where $\Psi_p$ is defined in \cite[2.22-2.23]{romisch_stability_2003}. Whenever $Q\in\mathcal{P}\left(\Xi\right)$ with finite $p$-th order absolute moments and $\zeta_p\left(P,Q\right)<\delta$
\end{theorem}
These results has two main points which can be seen as stability results. First, we understand that under a slight perturbation of $P$ (in terms of Fortet-Mourier metric), the optimal value of Problem \eqref{stochastic} is controlled. Second, the set of optimal $x$ of the perturbed model will remain close to the set of optimal $x$ of the original problem. Note that the parameter $p$ appears in the third assumption.

\subsection{Application to two-stage linear models}\label{two stage}
We consider a linear two-stage model with fixed recourse, this model is generically written as: 
\begin{equation}\label{lp stoch}
    \min_x\left\{c^Tx + \mathbb{E}_\xi\left(\inf_y q\left(\xi\right)^Ty \right)\: :\: Wy\left(\xi\right)=h\left(\xi\right)-T\left(\xi\right)x,\: y\left(\xi\right)\geq0,\: x\in X \right\}.
\end{equation}
Where $x$ is called the first-stage decision variable, $y$ the second. The general idea of this model is a first-stage decision has to be made before observing $\xi$. The distribution of $\xi$ is known so that $x$ is chosen to ensure the best average outcome. Then after observing $\xi$, the second-stage variable $y$ is determined. The term "fixed recourse" highlights the nature of $W$, a fixed matrix that does not depend (in that case) on $\xi$. See \cite{wets_stochastic_1974} for a complete survey on linear two-stage programs with fixed recourse. In order to fit in the Definition \ref{stochastic}, we define:
$$
F_0\left(x,\xi\right)=\begin{cases} 
  c^Tx + \phi\left(q\left(\xi\right), h\left(\xi\right) -T\left(\xi\right)x\right) & h\left(\xi\right)-     T\left(\xi\right)x \in \text{pos}\left(W\right), q\left(\xi\right) \in D, \\
  +\infty & \text{otherwise},
\end{cases}
$$
where $\text{pos}(W)=\left\{Wy, y\in\RR_+^m\right\}$, $D=\left\{u\in\RR^m:\left\{z\in\RR^r:W^Tz \leq u\right\}\ne \emptyset\right\}$ and  \\$\phi\left(u,t\right)=\inf\left\{ u^Ty : Wy=t, y\geq0\right\}$. \ref{lp stoch} can be rewritten as:
\begin{equation}\label{rewrite}
    \min_x\left\{\int_\Xi F_0\left(x,\xi\right)\text{d}P\left(\xi\right): x\in X\right\}. 
\end{equation}
One can note that $d=0$ in this formulation. Let us introduce two assumptions: 
\begin{assumption}\label{h1} For each $\left(x,\xi\right)\in X\times \Xi$ it holds that $h\left(\xi\right)- T\left(\xi\right)x\in \text{pos}\left(W\right)$, and $q\left(\xi\right) \in D$.
\end{assumption}
\begin{assumption}
\label{h2} Let the distribution $P$ be in $\mathcal{P}\left(\Xi\right)$ and has a finite second moment i.e. $\int_\Xi \lVert \xi\rVert^2\text{d}P\left(\xi\right) < \infty$.
\end{assumption}
\noindent Under these two assumptions, equivalence between Problem \ref{lp stoch} and Problem \ref{rewrite} is direct, even though it may look harsh, as both terms inside the integral converge then it is a matter of notation.
%\begin{proof}
    %Let $x\in X$. \Cref{h1} states that $F_0$ will always be written $c^Tx + \phi\left(q\left(\xi\right), h\left(\xi\right) -T\left(\xi\right)x\right)$ when we integrate it on $\Xi$ when $x\in X$.
    %\begin{align*}
    %    \int_\Xi F_0\left(x,\xi\right)\text{d}P\left(\xi\right)&=\int_\Xi c^Tx + \phi\left(q\left(\xi\right), h\left(\xi\right) -T\left(\xi\right)x\right)\text{d}P\left(\xi\right) \\
    %    &= c^Tx + \int_\Xi \inf_y\left\{q\left(\xi\right)^Ty\: :\: Wy=h\left(\xi\right) -T\left(\xi\right)x\right\}\text{d}P\left(\xi\right) \\
    %    &= \left\{c^Tx + \int_\Xi \inf_y\left\{q\left(\xi\right)^Ty\right\}\text{d}P\left(\xi\right)\:,\: Wy=h\left(\xi\right) -T\left(\xi\right)x\right\}
    %\end{align*}
    %There is no problem to split the integral in two as $c^Tx$ is finite, does not depend on $\xi$ and the whole integral is finite as well. We  used obviously $\int_\Xi\text{d}P\left(\xi\right)=1$. The third equality is a matter of notation. 
%\end{proof}

\begin{proposition}\label{prop 2}
    Let \Cref{h1} be satisfied. Then $F_0$ is a random convex function. Furthermore there exist constants $L>0, \bar{L}>0$, and $K>0$ such that the following holds for all $\left(\xi,\xi'\right)\in\Xi^2$ and $\left(x,x'\right)\in X^2$ with any $r>0$ such that $\max\left\{\lVert x\rVert, \lVert x' \rVert\right\}\leq r$, we have:
    \begin{align}
        \lvert F_0\left(x,\xi\right)- F_0\left(x,\xi'\right)\rvert &\leq Lr\max\left\{1,\lVert \xi\rVert, \lVert \xi'\rVert\right\}\lVert \xi-\xi'\rVert, \label{lips}\\
        \lvert F_0\left(x,\xi\right)- F_0\left(x',\xi\right)\rvert &\leq \bar{L}\max\left\{1,\lVert\xi\rVert^2\right\}\lVert x-x'\rVert, \nonumber \\
        \lvert F_0\left(x,\xi\right) \rvert&\leq Kr\max\left\{1,\lVert\xi\rVert^2\right\} \nonumber.
    \end{align}
     
\end{proposition}
\begin{proof}
    See \cite[Proposition 22]{romisch_stability_2003}.
\end{proof}
Thereafter we will work with finite distributions, implying that the support of $P$ will always be bounded. For our work, we can easily bound $\Xi$ w.l.o.g. Moreover, every distribution that we will create in this work have their support included in $\text{supp}\left(P\right)$ or in the convex hull of $\text{supp}\left(P\right)$. Thus we can assume that $\Xi$ is bounded, we can modify a little what is said in \Cref{lips} from \Cref{prop 2} to our convenience:
$$
\lvert F_0\left(x,\xi\right)- F_0\left(x,\xi'\right) \leq L'r \lVert \lVert \xi-\xi'\rVert, \text{ with } L'=L\max\left\{1,\max_{\xi\in\Xi}\lVert\xi\rVert\right\}.
$$
This means that there exists a constant $C>0$ such that $\frac{1}{C}F_0\left(x,\cdot\right)\in\mathcal{F}_1\left(\Xi\right)$ for each $x\in X$ hence if we fix $U$ an open, bounded neighbourhood of $X^*\left(P\right)$ the result hold for each $x\in X\cap \text{cl}U\subset X$. In other words, condition (3) of \Cref{stability_th} is verified with $p=1$.

\begin{corollary}
    Let \Cref{h1} and \Cref{h2} be satisfied, $\Xi$ be bounded, $X^*\left(P\right)$ be nonempty and $\mathcal{U}$ be an open, bounded neighbourhood of $X^*\left(P\right).$ Then there exist constants $L>0$ and $\delta >0$ such that:
    \begin{align*}
        \lvert v\left(P\right)-v\left(Q\right)\rvert \leq L\zeta_1\left(P,Q\right) \\
        \emptyset \ne X^*\left(Q\right)\subset X^*\left(P\right)+\Psi_P\left(L\zeta_1\left(P,Q\right)\right)\mathbb{B}
    \end{align*}
    whenever $Q\in\mathcal{P}\left(\Xi\right)$ and has a finite first moment and $\zeta_1\left(P,Q\right)<\delta$.
\end{corollary}
\begin{proof}
    This is an application of \Cref{stability_th} with $p=1$ where both assumptions are sufficient to ensure that conditions (2), (3) of \Cref{stability_th} are verified. Condition (1) is verified when the problem is well posed, meaning that there exists a solution. See \cite[Proposition 22]{romisch_stability_2003}.
\end{proof}
\begin{remark}
    In \cite{romisch_stability_2003}, $\Xi$ is not assumed to be bounded but the results hold with $p=2$.
\end{remark}
Consider now a two-stage model with a distribution $P=\sum_{i\in I}\delta_{x_i}p_i$ with $n$ atoms and $n$ is large. We may want to find another distribution $Q$ with a reduced number of atoms. We want $Q$ to be close to $P$ in terms of the distance $\zeta_1$ so that $\lvert v\left(P\right)-v\left(Q\right)\rvert$ is controlled. Finding such a reduced distribution $Q$ would reduce run time when solving the associated SP on a computer. Let us recall the Definition of the $p$-th order Fortet-Mourier metric:
$$
\zeta_1\left(P,Q\right)=\sup_{F\in\mathcal{F}_1\left(\Xi\right)}\lvert \int_\Xi F\left(\xi\right)\text{d}P\left(\xi\right)-\int_\Xi F\left(\xi\right)\text{d}Q\left(\xi\right)\rvert,
$$
where $\mathcal{F}_1\left(\Xi\right)=\left\{F:\Xi\to \RR: \lvert F\left(\xi\right)-F\left(\tilde{\xi}\right)\rvert \leq \lVert\xi-\xi'\rVert, \forall \left(\xi,\xi'\right)\in \Xi^2 \right\}$. Computing this upper bound on the Lipschitz functions tends to be difficult. We introduce $U\left(P,Q\right)$ the set of joint distributions whose marginals are $P$ and $Q$ and the type-$\ell$ Wasserstein distance: $$
W_\ell\left(P,Q\right) = \left\{\min_{\pi\in U\left(P,Q\right)}\int_{\Xi^2}\lVert \xi-\xi'\rVert^\ell \text{d}\pi\left(\xi,\xi'\right)\right\}^{1/\ell}.$$
Where $\lVert \cdot \rVert$ denotes the norm on $\Xi$ used to define $\mathcal{F}_1\left(\Xi\right)$. The next idea is to bound $\zeta_1$ by any $W_\ell$ which would be easier to compute.
\begin{align*}
    \zeta_1\left(P,Q\right)&= \sup_{F\in\mathcal{F}_1\left(\Xi\right)}\lvert \int_\Xi F\left(\xi\right)\text{d}P\left(\xi\right)-\int_\Xi F\left(\xi\right)\text{d}Q\left(\xi\right)\rvert, \\
    &=\mathcal{W}_1\left(P,Q\right), \quad \mathcal{W} \text{ being the dual of } W \text{, see \cite[Chapter 6]{peyre_computational_2019},}\\
    &= W_1\left(P,Q\right), \quad \text{due to the duality theorem of Kantorovich-Rubinstein.}
\end{align*}
Let $\ell >1$, $q>1$ such that $\frac{1}{\ell}+\frac{1}{q}=1$ and $\pi\in U\left(P,Q\right)$:
\begin{align*}
W_1\left(P,Q\right)&\leq \int_{\Xi^2}\lVert \xi-\xi'\rVert \text{d}\pi\left(\xi\times\xi'\right),\: \pi \text{ may not be optimal,} \\ &\leq \left(\int_{\Xi^2} \lvert 1\rvert^{q}\text{d}\pi\left(\xi\times\xi'\right)\right)^{1/q} \left(\int_{\Xi^2}\lVert \xi-\xi'\rVert^\ell\text{d}\pi\left(\xi\times\xi'\right)\right)^{1/\ell}, \: \text{due to Hölder's inequality} \\
&=\left(\int_{\Xi^2}\lVert \xi-\xi'\rVert^\ell\text{d}\pi\left(\xi\times\xi'\right)\right)^{1/\ell}.
\end{align*}
We can now take the infimum over $\pi$ what gives:
$$
W_1\left(P,Q\right)\leq W_\ell\left(P,Q\right).
$$
For a two-stage linear model, we have finally proven that for $\ell\geq1$, there exists a constant $L>0$ such that:
$$
\lvert v\left(P\right)-v\left(Q\right)\rvert \leq LW_\ell\left(P,Q\right).
$$
Thereafter we will consider a finite distribution $P$ and attempt to find another distribution $Q$ with fewer atoms with the idea of minimizing the type-$\ell$ Wasserstein distance between them. This would control the term: $\lvert v\left(P\right)-v\left(Q\right)\rvert$. The Wasserstein distance with $\ell\in\left\{1,2\right\}$ have been the most studied computationally because of their gentle properties. We will work with $\ell\in\left\{1,2\right\}$ for the theoretical part and $\ell=2$ for every simulation.

\section{$\quad$ Scenario reduction with Wasserstein distance on finite distributions}

In this part, we present tools and concepts useful for scenario reduction as well as their guarantees and limits. Results derive from \cite{rujeerapaiboon_scenario_2022}. 

\subsection{Introduction to Wasserstein distance}
We focus on the case of discrete distributions $P=\sum_{i\in I}p_i\delta_{x_i}$ and $Q=\sum_{j\in J}q_j\delta_{y_j}$. The type-$l$ Wasserstein distance between $P$ and $Q$ is defined as:  
\begin{definition}{Type-$\ell$ Wasserstein distance}
$$
d_\ell(P,Q)=\left(\min_{\pi\in\mathbb{R}_+^{\lvert I\rvert\times\lvert J\rvert}}\left\{ 
\sum_{i\in I}\sum_{j\in J}\pi_{ij}\lVert x_i-y_j\rVert^\ell \: \text{ : } \:  \begin{aligned}
& \sum_{j\in J}\pi_{ij}=p_i \: \forall i\in I \\
& \sum_{i\in I}\pi_{ij}=q_j \: \forall j\in J
\end{aligned}\right\}\right)^{1/\ell}.
$$
\end{definition}

\begin{remark}
    A more general definition can be written by replacing the norm with a particular cost between objects $x_i$ and $y_j$.
\end{remark}
For $\ell\geq1$, the functional $d_\ell$ meets the requirements of a distance. Values are positive, symmetry is direct, definiteness is more involved, see \ref{definiteness}. The triangle inequality derives from the gluing lemma, see e.g. \cite[Chapter 1]{peyre_computational_2019}. A pro of this particular distance is to rely on the underlying structure of the normed space and extend it to probability distributions supported on $X$. The philosophy of this distance is to measure the minimum cost of moving weights from $x_i$ to $y_j$ where costs are defined by the norm. For $\ell\in\mathopen{]}0,1\mathclose{[}, d_\ell^\ell$ is a distance.
\newline

Let $\mathcal{P}_U(X,n)$ denote the set of all \emph{uniform} discrete distribution on $X\subset R^d$ with $n$ distinct scenarios and $\mathcal{P}(X,m)$ denote the set of discrete distributions on $X\subset R^d$ with \emph{at most} m scenarios. 
\newline

As presented in the conclusion of \Cref{stability}, we want to find a distribution that minimizes the Wasserstein distance over a particular set of distribution. We study two types of scenario reduction. The continous scenario reduction problem is:
$$
C_\ell(P,m)=\min_Q\left\{d_\ell(P,Q),\: Q\in\mathcal{P}\left(\mathbb{R}^d,m\right)\right\}, 
$$
and the discrete scenario reduction problem is:
$$
D_\ell(P,m)=\min_Q\left\{d_\ell(P,Q),\: Q\in\mathcal{P}(\text{supp}\left(P\right),m)\right\}.
$$

Let $P$ be a finite distribution on $\RR^d$. As $\text{supp}\left(P\right)\subset \RR^d$, it follows that $C_\ell\left(P,m\right)\leq D_\ell\left(P,m\right)$. Discrete scenario reduction may be sometimes more suitable as continous scenario reduction can generate atoms that may lack interpretability. For example in \ref{section mpa}, a non-integer demand of screw can be weird.

\subsection{Comparative study of continuous and discrete scenario reduction}\label{comparative}

In this part, we provide bounds on the Wasserstein distance between an original distribution of $n$ atoms and its reduced one with $m\ll n$ atoms. Atoms of the reduced distribution are either free in $\RR^d$ in the case of continuous scenario reduction or in $\text{supp}\left(P\right)$ in the discrete case. Let $X$ be a subset of $\RR^d$ and $\lVert \cdot \rVert$ be a norm on $X$. We will work with uniform distribution $P\in\mathcal{P}_U\left(\RR^d,n\right)$ with the idea that the distributions derive from sampling. If not, intuitively one can break down masses of high probability into many atoms. The following figure \Cref{break down} illustrates this remark. The red crosses represent atoms of the original distribution and sizes the probability of an atom. \clearpage

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.40\linewidth]{plots/break down.pdf}
    \caption{How to break down masses}
    \label{break down}
\end{figure}

We also restrict ourselves to distribution whose atoms lie within the unit ball and $\frac{1}{n}\sum_ix_i=0$. This is a valid assumption because first $C_\ell$ and $D_\ell$ are invariant under translation, second $C_\ell$ and $D_\ell$ are positively homogeneous. Let $P=\frac{1}{\lvert I\rvert}\sum_{i\in I}\delta_{x_i}$, $\text{card}\left(I\right)=n<+\infty$, $1<m<n$, $\lambda\in\RR^*$ and $a\in\RR^d$ \rp{faire une appendice}:
$$ 
C_\ell\left(P,m\right)=\frac{1}{\lvert\lambda\rvert} C_\ell\left(\sum_{i\in I}p_i\delta_{\lambda x_i+a},m\right).
$$
The result hold when swapping $C$ by $D$. See Appendix \ref{appendix restrict} and Appendix \ref{appendix homogen}. A goal of this part is to quantify and study the worst case of scenario reduction defined by:
$$    \bar{C}_\ell\left(n,m \right)=\left\{\max_{P\in\mathcal{P}_U(\mathbb{R}^d,n)}C_\ell(P,m)\: :\: \text{supp}(P)\in B(0,1)\right\}.
$$
Let us write a first bound that we will tighten thereafter, let $P\in\mathcal{P}_U(\mathbb{R}^d,n)$:
\begin{equation}\label{easy bound}C_\ell\left(P,m\right)\leq C_\ell\left(P,1\right)\leq d_\ell\left(P,\delta_{0_{\RR^d}}\right)\leq 1.\end{equation}
As the right-hand side of \Cref{easy bound} does not depend on $P$, we can take the upper bound over $P\in\mathcal{P}_U\left(\RR^d,n\right)$ and write that:
$$
\bar{C}_\ell\left(n,m \right)\leq1.
$$
Let $\mathfrak{P}(I,m)$ be the family of m-set partitions of $I$. Following \cite{rujeerapaiboon_scenario_2022}, we write $\{I_j\}$ an element of $\mathfrak{P}(I,m)$ and $I_j$ for $j\in\{1,..,m\}$ a set of the partition $\{I_j\}$.
\begin{theorem}{Reformulation}\label{theorem1}
$$C_\ell(P,m)=\min_{\{I_j\}\in \mathfrak{P}(I,m)}\left\{ \frac{1}{n}\sum_{j\in J}\min_{y_j\in\mathbb{R}^d}\sum_{i\in I_j}\lVert x_i-y_j\rVert^\ell \right\}^{1/\ell}.$$
\end{theorem}
\begin{remark}
    This reformulation finds his roots of \Cref{closed formula}.
\end{remark}
Assume that $X$ is endowed with the euclidean norm. In the case $\ell=1$, $y_j^*$ is attained by any geometric median. Now if $\ell=2$, the inner minimum can be found easily, gradient of the inner function is well-defined and minimizing with  no constraints gives: $$y_j^*=\text{mean}\left(I_j\right)=\frac{1}{\lvert I_j\rvert}{\sum_{i\in I_j}x_i}.$$ 
\begin{proposition}{Tighter bounds}
$$\bar{C_1}\left(n,m\right)\leq\bar{C_2}\left(n,m\right)\leq\sqrt{\frac{n-m}{n-1}}.$$
\end{proposition}
\begin{proof}
    See \cite[Theorem 2 and 3]{rujeerapaiboon_scenario_2022}.
\end{proof}
Reduced distributions obtained via continuous scenario reduction are always at least as good as reduced sets obtained via discrete scenario reduction. We will try to quantify the gap between the solution of both problems. More precisely, in this part we find for $\ell\in\{1,2\}$: 
$$\underline{\kappa_\ell}\left(n,m\right)\cdot C_\ell\left(P,m\right)\leq D_\ell\left(P,m\right)\leq \bar{\kappa_\ell}\left(n,m\right)\cdot C_\ell\left(P,m\right).$$
The goal is to find the largest $\underline{\kappa_\ell}\left(n,m\right)$ and smallest $\bar{\kappa_\ell}\left(n,m\right)$ so the bounds are as tight as possible. Solving the continuous problem will give bounds on the discrete. It is easy to argue that: 
$$
\underline{\kappa_\ell}\left(n,m\right)\geq1.
$$
Because we have already proven: 
$$
C_\ell\left(P,m\right)\leq D_\ell\left(P,m\right).
$$
Similarly to what we have done just before, it is time to reformulate the discrete scenario reduction problem such as:
\begin{theorem}\label{reformulation 2}
    For any type-l Wasserstein distance induced by any norm $\lVert\cdot\rVert$, the discrete scenario reduction problem admits the following reformulation: 
    $$
    D_\ell\left(P,m\right)=\min_{\{I_j\}\in\mathfrak{P}\left(I,m\right)}\left[ \frac{1}{n}\sum_{j\in J}\min_{y_j\in\{x_i : i\in I_j\}}\sum_{i\in I_j}\lVert x_i-y_j\rVert^\ell\right]^{1/\ell}.
    $$
\end{theorem}

\noindent What follows assumes that $n\geq2, m\in\{1,...,n-1\}$ and $d\geq2.$ \\
\paragraph{\textbf{Type-2 Wasserstein distance.}}
\begin{proposition}
    The upper bound $\bar\kappa_2\left(n,m\right)$ satisfies $\bar\kappa_2\left(n,m\right)\leq \sqrt{2}$ for all n, m.
\end{proposition}
\begin{proof} 
    The proof is very interesting for one's intuition. \textbf{In the first step}, we prove the result for $m=1$. Let $P=\frac{1}{n}\sum_{i\in I}\delta_{x_i}\in\mathcal{P}_U\left(\RR^d,n\right)$. As presentend in the introduction of \ref{comparative}, we can assume w.l.o.g. that $\frac{1}{n}\sum_{i\in I}x_i=0$ and $\frac{1}{n}\sum_{i\in I}\lVert x_i\rVert^2=1$.
    \newline
    The first reformulation of \Cref{theorem1} gives:
    $$
    C_2\left(P,1\right)=\left[\frac{1}{n}\sum_{i\in I}\lVert x_i-\text{mean}(I)\rVert_2^2\right]^{1/2}=1.
    $$
    Now we use the reformulation of \Cref{reformulation 2} in order to write:
    $$
    D_2\left(P,1\right)=\left[\frac{1}{n}\min_{j\in I}\sum_{i\in I}\lVert x_i-x_j\rVert^2_2\right]^{1/2}.
    $$
    The $\left(j, x_j\right)$ minimizing this expression minimizes the sum of squared distances with all atoms of $P$. Let us develop:
    \begin{align*}
    D_2\left(P,1\right)&=\left[\frac{1}{n}\min_{j\in I}\sum_{i\in I}\left(\rVert x_i\rVert^2_2-2x_i^Tx_j+\rVert x_j\rVert^2_2\right)\right]^{1/2}\\&=\left[\frac{1}{n}\sum_{i\in I}\lVert x_i\rVert_2^2-\frac{2}{n}\min_{j\in I}\left(\sum_{i\in I}x_i^T\right)x_j+\min_{j\in I}\lVert x_j\rVert^2_2\right]^{1/2} \\&=\left(1+\min_{j\in I}\lVert x_j\rVert^2_2\right)^{1/2}\leq\sqrt{2}.
    \end{align*}
Where we have used $\text{mean}(I)=0$ to write the third equality. The final inequality is due to: $$\min_{j\in I}\lVert x_j\rVert^2_2=\frac{1}{n}\sum_{i\in I}\min_{j\in I}\lVert x_j\rVert^2_2\leq \frac{1}{n}\sum_{i\in I}\lVert x_j\rVert^2_2=1.$$ \noindent Then we have: $$D_2\left(P,1\right)\leq\sqrt{2}$$
In order to make it complete for the case $m=1$. Let $P=\frac{1}{n}\sum_{i\in I}\delta_{x_i}\in\mathcal{P}_U\left(\RR^d,n\right)$. We consider $P'=\frac{1}{n}\sum_{i\in I}\delta_{\frac{x_i-\text{mean}(I)}{r}}$ with $r=\sqrt{\sum_{i\in I}\rVert x_i-\text{mean}(I)\lVert^2_2}$. $P'$ verifies the aforementionned conditions. Then we have: 
$$
\frac{D_2\left(P,1\right)}{C_2\left(P,1\right)}=\frac{rD_2\left(P',1\right)}{rC_2\left(P',1\right)}=\frac{D_2\left(P',1\right)}{C_2\left(P',1\right)}\leq \sqrt{2}.
$$
Finally, we have: $$\bar\kappa_2\left(n,1\right)\leq\sqrt{2}.$$
\newline
\textbf{Step 2} extends the result for $n,m\in\{1,...,n-1\}$ using the result above for $m=1$ by decomposing with $m$ scenario reduction problem with 1 atom. Let $P\in\mathcal{P}_U\left(\RR^d,n\right). $ With \Cref{theorem1}, we can write that:
$$
C_2\left(P,m\right)=\min_{\{I_j\}\in \mathfrak{P}(I,m)}\left\{ \frac{1}{n}\sum_{j\in J}\sum_{i\in I_j}\lVert x_i-\text{mean}\left(I_j\right)\rVert^2_2 \right\}^{1/2}.
$$
For an optimal $\{I_j^*\}\in\mathfrak{P}\left(I,m\right)$, we have that:
$$
C_2\left(P,m\right)=\left(\frac{1}{n}\sum_{j\in J}\sum_{i\in I_j^*} \lVert x_i-\text{mean}\left(I_j^*\right)\rVert^2_2\right)^{1/2}=\left(\frac{1}{n}\sum_{j\in J}\lvert I_j^*\rvert C^2_{2,j}\right)^{1/2}.
$$
with $C_{2,j}=\left(\frac{1}{\lvert I_j^*\rvert}\sum_{i\in I_j^*}\lVert x_i -\text{mean}\left(I_j\right)\rVert^2_2\right)$. So typically,  $C_{2,j}=C_2\left(\frac{1}{\lvert I_j^*\rvert}\sum_{i\in I_j^*}\delta_{x_i},1\right)$ as reminded for \textbf{step 1}.

\noindent With $D_{2,k}=\min_{i\in I_k}\left(\frac{1}{\lvert I_k\rvert}\sum_{j\in I_k}\lVert x_j -x_i\rVert^2_2\right)^{1/2}$ and \Cref{reformulation 2}, we have the analogous expression for an optimal $\{I_k^*\}\in\mathfrak{P}\left(I,m\right)$ : 
$$
D_2\left(P,m\right)=\left[ \sum_{k\in K}\frac{\lvert I_k^*\rvert}{n}D_{2,k}^2\right]^{1/2}\leq \left[ \sum_{j\in J}\frac{\lvert I_j^*\rvert}{n}D_{2,j}^2\right]^{1/2}\leq \left[ \sum_{j\in J}\frac{\lvert I_j^*\rvert}{n}2C_{2,j}^2\right]^{1/2}=\sqrt{2}C_2\left(P,m\right).
$$
The first inequality stands because $I_j^*\in\{I_j^*\}$ is an optimal partition for $C_2$, thus it is worse than or equal to $I_k^*$ for $D_2$. The second inequality stems from \textbf{step 1}. The statement is thus proven.
\end{proof}
\begin{proposition}
    $\bar\kappa_2\left(n,m\right)=\sqrt{2}$, meaning that there exists $P\in\mathcal{P}_U\left(\RR^d,n\right)$ such that $D_2\left(P,m\right)=\sqrt{2}C_2\left(P,m\right)$
\end{proposition}

Lower bounds are given as follows : 
\begin{proposition}
    The lower bound $\underline\kappa_2\left(n,m\right)=1$ whenever $n\geq3$ and $m\in\{1,...,n-2\}$ while $\underline\kappa_2\left(n,n-1\right)=\sqrt{2}.$
\end{proposition}
\paragraph{\textbf{Type-1 Wasserstein distance.}}
Similarly to the Type-2 Wasserstein distance, we intend to find $\bar{\kappa}_1\left(n,m\right), \underline\kappa_1\left(n,m\right)$. In this part, we will only write results. See \cite[Section 3.2]{rujeerapaiboon_scenario_2022} for proofs.
\begin{theorem}
    The upper bound $\bar\kappa_1\left(n,m\right)$ satisfies $\bar\kappa_1\left(n,m\right)\leq2$ whenever $m\in\{2,...,n-2\}$ as well as $\bar\kappa_1\left(n,1\right)\leq2\left(1-\frac{1}{n}\right)$ and $\bar\kappa_1\left(n,n-1\right)\leq1$
\end{theorem}
\begin{proposition}
    There is $P\in\mathcal{P}_U\left(\RR^d,n\right)$ such that $D_1\left(P,m\right)=2\left(1-\frac{1}{n}\right)C_1\left(P,m\right)$ under the 1-norm for all $n$ divisible by $2m$, all $m$ and all $d\geq \frac{n}{2m}$.
\end{proposition}
\begin{proposition}
    The lower bound $\underline{\kappa}_1\left(n,m\right)$ satisfies $\underline{\kappa}_1\left(n,m\right)=1$ for all $n, m$.
\end{proposition}
\section{$\quad$ Algorithms for scenario reduction}
In this section, we present various algorithms and heuristics that are often used for scenario reduction problem. In order to check the accuracy of algorithms, we write the output of an algorithm $\bar{D_\ell}\left(P,m\right)$ which is an upper bound of   $D_\ell\left(P,m\right)$. We then define the \emph{approximation ratio} by: 
$$
\max_{n,m,P\in\mathcal{P}_U(\RR^d,n)}\frac{\bar{D}_\ell\left(P,m\right)}{D_\ell\left(P,m\right)}.
$$
Naturally this approximation ratio is greater than or equal to 1 as $\bar{D}_\ell\left(P,m\right)\geq D_\ell\left(P,m\right)$. A bounded approximation ratio would ensure a proximity to optimality for the reduced scenario set obtained via this particular algorithm. On the contrary an unbounded ratio for a particular algorithm means that there exist distributions whose reduced distribution can be arbitrary far from the optimal.
\subsection{Redistribution rule}\label{redistribution rule}
In this part, we write and make the proof of an useful result that we will use in most algorithms.
\begin{theorem}\label{closed formula}
    Let $P=\sum_{i\in I}p_i\delta_{x_i}$ be a fixed distribution, $Q\left(q\right)=\sum_{j\in J}q_j\delta_{y_j}$ have its atoms $y_j$ fixed and its weights $q_j$ are variable. Then we have that: 
    $$Q^*=\sum_{j\in J}q_j^*\delta_{y_j}\in \argmin_{Q\left(q\right)} d_\ell\left(P,Q\left(q\right)\right).$$
    Where $C_j=\left\{i\in I \mid j=\argmin_{k\in J} \lVert x_i-y_k\rVert^\ell\right\}$ representing the index $i$ of the closest atom of $X$ for $y_j$ and $q_j^*=\sum_{i\in C_j}p_i$. Ties must be broken so that the argmin defining $C_j$ is a singleton.
\end{theorem}
\begin{proof}
Let us write carefully the problem, we intend to solve:
$$
    \min_{q\geq0}\:\left\{d_\ell\left(P,\sum_{j\in J}q_j\delta_{y_j}\right)\text{ such that} \;\sum_{j\in J}q_j=1 \right\}.
$$

We will first find a lower bound of that expression, then we will prove that the lower bound is attained by $Q^*$. Let us write $U\left(p,q\right)=\left\{\pi\in\RR^{\lvert I\rvert\times\lvert J\rvert}_+,\: \sum_{i\in I}\pi_{ij}=q_j,\:\sum_{j\in J}\pi_{ij}=p_i\right\}$ the admissible set for $\pi$ defined in the Wasserstein distance definition.
\newline
    
Let $q\geq 0$ such that $\sum_{j\in J}q_j=1$. We write $Q\left(q\right)=\sum_{j\in J}q_j\delta_{y_j}$ and $P=\sum_{i\in I}p_i\delta_{x_i}$. We have that:
\begin{align}
    d_\ell\left(P,Q\left(q\right)\right)^\ell&= \min_{\pi\in U\left(p,q\right)}\sum_{i\in I}\left[\sum_{j\in J}\pi_{ij}\lVert x_i-y_j\rVert^\ell\right] \nonumber \\
    &\geq \min_{\pi\in U\left(p,q\right)}\sum_{i\in I}\left[\sum_{j\in J}\pi_{ij}\min_{k\in J}\left\{\lVert x_i-y_k\lVert^\ell\right\}\right] \label{pi posi} \\ &= \min_{\pi\in U\left(p,q\right)}\sum_{i\in I}\min_{k\in J}\left\{\lVert x_i-y_k\lVert^\ell\right\}p_i \label{no pi}
    \\& =\sum_{i\in I}\min_{k\in J}\left\{\lVert x_i-y_k\lVert^\ell\right\}p_i. \label{low bound}
\end{align}
    
The first inequality \ref{pi posi} stems from the positivity of $\pi$ and for every $j\in J, \lVert x_i-y_j\rVert^\ell \geq \min_{k\in J}\lVert x_i-y_j\rVert^\ell$. The before last expression \ref{no pi} no longer depends on $\pi$ then comes the last expression. Line \ref{low bound} is our lower bound as it no longer depends on $q_j$, we can minimize on $q$:
\begin{equation}\label{lower}\min_{q}d_\ell\left(P,Q\left(q\right)\right)^\ell\geq\sum_{i\in I}\min_{k\in J}\left\{\lVert x_i-y_k\lVert^\ell\right\}p_i.
\end{equation}

\noindent We write now $\delta^K$ the Kronecker delta which is defined as: $$\delta^K_{ij} = \begin{cases} 
      1 & \text{if } i=j \\
      0 & \text{otherwise. } 
   \end{cases}.$$
Let us write a transport plan $\pi^*$ to move the distributuion $P$ to the distribution $Q^*$,
\begin{align*}
    q_j^*&=\sum_{i\in C_j}p_i, \\
    \pi_{ij}^*&=p_i\delta^K_{j,\argmin_{k\in J}\lVert x_i-y_k\rVert^\ell}.
\end{align*}
\noindent This transport plan $\pi^*$ is chosen with the idea of moving the mass $p_i$ of $x_i$ to the closest atom that is in the supp$\left(Q\right)$. $\pi^*$ is positive. Knowing that the sets $\{C_j\}_{j\in J}$ represents a partition of $I$ (as ties are broken), let us verify that $\pi^*\in U\left(P,Q^*\right)$:
\begin{align*}
    &\sum_{j\in J}\pi^*_{ij}=p_i\sum_{j\in J}\delta^K_{j,\argmin_{k\in J}\lVert x_i-y_k\rVert^\ell}=p_i, \\
    &\sum_{i\in I}\pi^*_{ij}=\sum_{i\in I}p_i\delta^K_{j,\argmin_{k\in J}\lVert x_i-y_k\rVert^\ell}=\sum_{i\in C_j}p_i=q_j^*.
\end{align*} $\pi^*$ is admissible. The value of the objective function is: 
\begin{align*}
    \sum_{i\in I}\sum_{j\in J}\pi^*_{ij}\lVert x_i-y_j\rVert^\ell &=\sum_{i\in I}\sum_{j\in J}p_i\delta^K_{j, \argmin_{k\in J}\lVert x_i-y_k\rVert^\ell}\lVert x_i-y_j\rVert^\ell \\ &=\sum_{i\in I}p_i\left[\sum_{j\in J}\delta^K_{j, \argmin_{k\in J}\lVert x_i-y_k\rVert^\ell}\lVert x_i-y_j\rVert^\ell\right] \\ &=\sum_{i\in I}p_i\min_{k\in J}\lVert x_i-y_k\rVert^\ell.
\end{align*}
All in all, we have proven that:
$$
\sum_{i\in I}p_i\min_{k\in J}\lVert x_i-y_k\rVert ^\ell\overset{I(1)}{\leq} \min_{q} d_\ell^\ell\left(P,Q\left(q\right)\right)\overset{I(2)}{\leq} d_\ell^\ell\left(P,Q^*\right) \overset{I(3)}{\leq} \sum_{i\in I}p_i\min_{k\in J}\lVert x_i-y_k\rVert ^\ell.
$$
The first inequality $I(1)$ is due to \ref{lower}. The second $I(2)$ comes from the fact that we fixed $q_j$ for $Q^*$. The third $I(3)$ stems from the definition of $d_\ell$ because we chose a particular $\pi^*\in U\left(p,q\right)$. Then we have that:
$$
\min_{q} d_\ell^\ell\left(P,Q\left(q\right)\right)= d_\ell^\ell\left(P,Q^*\right)= \sum_{i\in I}p_i\min_{k\in J}\lVert x_i-y_k\rVert ^\ell.
$$
Finally, we have shown that:
$$\sum_{j\in J}\left(\sum_{i\in I_j}p_i\right)\delta_{y_j}\in \argmin_q d_\ell\left(P,Q\left(q\right)\right).$$
\end{proof}
\begin{remark}
    A proof of \Cref{closed formula} can be found in \cite{dupacova_scenario_2003} based on duality. The original proof given here is only based on primal arguments.
\end{remark}

\subsection{Dupačová et al. algorithm to approximate discrete scenario reduction}
Dupačová et al. in \cite{dupacova_scenario_2003} provides a greedy algorithm for $D_\ell\left(P,m\right)$. Generally, the distribution obtained is not a solution of the discrete scenario reduction problem. We write $D_\ell\left(P, X\right)$ where $X$ represents a set of atoms the value: $\min_{q}d_\ell\left(P,\sum_{i=1}^{\lvert X\rvert} q_jx_j\right)$.

\begin{algorithm}
  \caption{Dupačová et al.}\label{dupacova}
  Initialize the set of atoms in the reduced set as $R\gets \emptyset.$ \\ Select the next atom to be added to the reduced set as: $$ y\in\argmin_{y\in \text{supp}\left(P\right)}D_\ell\left(P,R\cup\{y\}\right).
  $$ \\
  Update $R\gets R\cup \{y\}$.\\ Repeat step 2 and 3 until $\lvert R\rvert=m$.
\end{algorithm}

\noindent Step 2 is not direct. The main thing is to find the optimal distribution supported on $R\cup\{y\}$. The redistribution rule that has been presented and proven in the section \ref{redistribution rule} allows us not to rely on solving a linear program. With \Cref{closed formula}, we found an efficient way to compute step 2: 
$$
D_\ell\left(P,R\cup \{y_j\}\right)=\sum_{i\in I}p_i\min_{z\in R\cup\{y\}}\lVert x_i-z \rVert^\ell.
$$
Assuming you know the vector $\left(\min_{z\in R}\lVert x_i-z \rVert^\ell\right)_{i\in I}$, you can compute each component of \newline$\left(\min_{z\in R\cup\{y\}}\lVert x_i-z \rVert^\ell\right)_{i\in I}$ with the following formula:
$$
    \min_{z\in R\cup\{y\}}\lVert x_i-z \rVert^\ell=\min\left\{\min_{z\in R}\lVert x_i-z \rVert^\ell, \lVert x_i-y \rVert^\ell\right\}.
$$
To make it cristal clear, let us write how that method works in practice. At iteration $0$, we store the distance matrix: 
$$
D = \left(\lVert x_i-x_j\rVert ^l \right)_{i,j}.
$$
Then we can choose the first atom $x$ of index $i_1$ to be added to $R$ as:
$$
i_1 \in \argmin_{i\in I} pD^i.
$$
Where $D^i$ is the $i^{th}$ column of $D$ and $p=\begin{pmatrix}
    p_1 & \hdots & p_n
\end{pmatrix}$.
We create $m^1=D^{i_1}$. For the second iteration, we can only select the indexes $[\![1;n]\!]\setminus \{i_1\}$. For each $i \in [\![1;n]\!]\setminus \{i_1\}$, let us create a vector $m_i$ such that $\left(m_i\right)_j=\min\left\{\left(m^1\right)_j,\left(D^i\right)_j\right\}$, now we can write $D_\ell$ as: $$
D_\ell\left(P,\{x_{i_1}\}\cup \{x_i\}\right)=pm_i.
$$
Then follows the greedy algorithm we expect to be efficient:
\begin{algorithm}
\caption{Dupačová et al. in precision}
Initialize the set of atoms in the reduced set as $R \gets \emptyset$.\\
Compute the matrix $D = \left(\| x_i - x_j \|^\ell\right)_{i,j \in I^2}$ once and for all.\\
Create $m^0 = \begin{bmatrix} a & \hdots & a \end{bmatrix}$ with $a$ arbitrarily large (choose $a \geq \max_{i,j} \lVert x_i - x_j \rVert^\ell$).\\
\While{$|R| < m$}{
    Select the next atom to be added from $I \setminus R$ by following this procedure.\\
    \For{$i \in I \setminus R$}{
        Compute $\left(m_i\right)_j = \min \left\{ \left(m^0\right)_j, \left(D^i\right)_j \right\}$.\\
        Compute $D_\ell(P, R \cup \{x_i\}) = p m_i$.\\
        Store the best $m_i$ so far, named $m_{\text{best}}$.\\
    }
    Update $m^0 = m_{\text{best}}$. \\
    Update $R = R \cup \{x_{\text{best}}\}$.\\
}
\Return $R$\\
\end{algorithm}
\begin{proof}\label{complexity}
The complexity of this algorithm is $\mathcal{O}\left(mn^2\right)$. First computing $D$ requires $\mathcal{O}\left(n^2\right)$ operations, then we will have $m$ iterations in the while loop. The procedure requires: $n-k$ for the $k^{th}$ iterations, computing $m_i$ requires $2n$ operations, $2n$ more operations to compute and 1 operation to test. Updating requires 1 operation.
Precisely, we make the following number of operations:
$$
n^2+\sum_{k=1}^m\left(2+\left(n-k\right)\left(2n+2n+1\right)\right)= n^2+2m+\left(4n+1\right)\left(mn-\frac{m\left(m+1\right)}{2}\right).
$$
\end{proof}
There also exist a backward version of this algorithm where the reduced set of atoms is initialized by $R\gets I$ and atoms are removed in order to minimize the Wasserstein distance. Unfortunately, this algorithm can not benefit from the easy computation of $D_\ell\left(P,k\right)$ with $D_\ell\left(P,k-1\right)$. Hence, it may be not as efficient as the forward version. Still, this backward version may be interesting when you do not want to reduce a lot the original distribution.
\subsection{Local-search algorithm}\label{guarant}
Adapting local-search to the discrete scenario reduction problem has the advantage of bounding the approximation ratio under the type-1 Wasserstein distance for both the continuous and discrete scenario reduction problem. The approximation ratio is bounded by 5 for the discrete scenario reduction problem and 10 for the continuous.
\begin{algorithm}
    \caption{Local search algorithm for $D_\ell\left(P,m\right)$}\label{Local search}
    Initialize the reduced set $R\subseteq \text{supp}\left(P\right), \lvert R\rvert = m$, arbitrarily. \\ Select the next change to be applied to the reduced set as: 
    $$
    \left(y,y'\right)\in\argmin\left\{D_\ell\left(P,R\cup\{y\}\setminus \{y'\}\right) : \left(y,y'\right)\in\text{supp}\left(P\setminus R\right)\times R\right\},
    $$
    and update $R\gets R\cup \{y\}\setminus \{y'\}$ if $D_\ell\left(P, R\cup\{y\}\setminus \{y'\} \right)<D_\ell\left(P,R\right).$ \\ Repeat Step 2 until no further improvement is possible.
\end{algorithm}

If various changes are possible in Step 2, you can either choose to use a "first-fit" (swap whenever you find a couple that reduces the distance) or a "best-fit" (explore all possible exchanges and consider the optimal) strategy.

We now use what we found for $\bar\kappa_1$ to state that this algorithm gives rise to an algorithm with approximation ratio : $2\times 5$. Let $n,m,P\in\mathcal{P}_U\left(\RR^d,m\right)$, we have that : 
$$
\frac{\bar{D}_\ell\left(P,m\right)}{C_\ell\left(P,m\right)}\leq \frac{2\bar{D}_\ell\left(P,m\right)}{D_\ell\left(P,m\right)}\leq 2\max_{n,m,P\in\mathcal{P}_U(\RR^d,n)}\frac{\bar{D}_\ell\left(P,m\right)}{D_\ell\left(P,m\right)}=2\times5=10.
$$
We can now maximize the left hand side on $n,m,P$ as the right hand side does not depend on the three.
The same approach does not work for the local search as you swap atoms. An idea is to store an index vector which tells you which atom reaches $\min_{x\in R}\lVert x_i-x\rVert^\ell$ so that you know when you have to compute again the minimum on $R$(whenever the swap removes the atom reaching the minimum. The algorithm in precision is written in \Cref{loc s}. There is still a lot of computation here in order to compute the minimum inside the closed formula, we have to go through the index vector and after that through $m-1$ columns of a matrix in order to find the new $\min_{i\in R\setminus x_j}\lVert x_k-x_i\rVert^\ell$ what has a complexity $\mathcal{O}\left(mn\right)$).
\clearpage
\begin{algorithm}
\caption{Local-search in precision}\label{loc s}
Initialize the set of atoms in the reduced set $R, \lvert R\rvert=m$.\\
Compute the matrix $D = \left(\| x_i - x_j \|^l \right)_{i,j \in I^2}$ once and for all.\\
Create $m^0 = \begin{pmatrix}\min_{i\in R}\lVert x_1-x_i\rVert^l & \hdots & \min_{i\in R}\lVert x_1-x_i\rVert^l \end{pmatrix}$ and $I=\begin{pmatrix}\argmin_{i\in R}\lVert x_1-x\rVert^l & \hdots & \argmin_{i\in R}\lVert x_1-x\rVert^l \end{pmatrix}$.\\
Create $d=pm^0$, there is improvement. \\
\While{\text{ There is improvement}}{
    There is not improvement. \\
    \For{$i \in R$}{
    Make a copy of I in index.\\
    Make a copy of $m^0$ in m. \\
        \For{$k\in I$}{
            \If{index$\left[k\right]=i$}{
                Change $\left[m\right]_k$ by $\min_{\alpha\in R\setminus\{i\}}\lVert x_k-x_\alpha\rVert^l$ and $index\left[k\right]$ as well.
            }
        }
        \For{$j\in I\setminus R$}{
            Compute $\left(m_{ij}\right) = \min \left\{ m, D^j \}\right\}$ where $\min$ is the minimum component-wise.\\ Compute $D_l(P, R \cup \{x_i\}) = p m_{ij}$.\\
            \If{$pm_{ij} < d$}{
                $d=pm_{ij}$ and there is improvement ! \\
                You can exit the \textbf{for} $i\in R$ loop if you want first-fit with , or you can keep going the for loop until you find the best-fit as $d$ is now worth $pm_{ij}$.
            }
        }
        Store the best $m_{ij}$ so far, named $m_{\text{best}}$.\\
    }
    \If{There is improvement}{
    Update $m^0 = m_{\text{best}}$. \\
    Update $R = R \cup \{x_j\} \setminus \{x_i\}$.}
}
\end{algorithm}

\subsection{MILP formulation}
Let us review a mixed-integer linear programming (MILP) reformulation of the discrete scenario reduction problem.
\begin{theorem}
The discrete scenario reduction problem can be formulated as: 
\begin{align*}
    D_\ell^\ell\left(P,m\right)=&\min_{\Pi,\lambda}\:\frac{1}{n}<\pi,D> \\ &\text{s.t. } \;\pi e=e, \; \pi \leq e\lambda^T,\; \lambda^Te=m,\\&\quad\quad \pi\in\RR^{n\times m}_+, \; \lambda\in\{0,1\}^n.
    \end{align*}
    with $D\in \mathbb{S}^n$ and $D_{ij}=\lVert x_i-x_j\rVert^\ell$.
\end{theorem}
\begin{remark}
    Once again, the reformulation derives from \Cref{closed formula}.
\end{remark}
\subsection{$k$-means}
The aim of $k$-means clustering is to reduced $n$ observations to $k$ clusters such that the intra-cluster sums of distances are minimized. It turns out to be a good idea to select $m$ atoms with the $n$ of $\text{supp}\left(P\right)$. The amount of atoms in a cluster will give the probability of observing the cluster for the reduced distribution.

\begin{algorithm}
    \caption{$m$-means clustering for $C_\ell\left(P,m\right)$}\label{k m}
    Initialize the reduced set $R=\{y_1,...,y_m\} \subseteq \text{supp}\left(P\right)$ arbitrarily \\ Let $\{I_j\}\in\mathfrak{P}\left(I,m\right)$ be any partition whose sets $I_j, j\in J$, contain all atoms of supp$\left(P\right)$ that are closest to $y_j$ (ties may be broken arbitrarily). \\ For each $j\in J$, update $y_j$ as: $$y_j \gets \argmin_{y\in\RR^d} \left\{ \sum_{i\in C_j}\lVert x_i-y\rVert^\ell\right\}.$$ \\ Repeat Steps 2 and 3 until the reduced set $R$ no longer changes.
\end{algorithm}
\begin{remark}
    When we use the euclidean norm and $\ell=2$, $y_j$ will be written as a mean on a subset $C_j$. When these conditions are not verified, you can use your preferred minimization technique.
\end{remark}
\noindent The reduced distribution is:
$$
Q=\frac{1}{n}\sum_{y\in R}\delta_{y}c\left(y\right),
$$
where $ c\left(y\right)$ denotes the amount of atoms of $P$ in the cluster $y$ because we have considered $P$ uniform.

\subsection{A new algorithm for continuous scenario reduction}
Dupačová et al. algorithm and local-search methods gives a great approximation of $D_\ell\left(P,m\right)$, we know also the distance can be written as:
$$D_\ell\left(P,R\right)=\sum_{i\in I}p_i\min_{j\in R}\lVert x_i-x_j\Vert^\ell=\sum_{j\in R}\left[\sum_{i\in I_j}p_i\lVert x_i-x_j\rVert^\ell\right].$$
Now we may improve the value of the Wasserstein distance by finding an optimal $x_j$ for each $I_j$. The idea is to decompose each sum on $I_j$ as we have done above in order to find for $I_j$: 
$$
y_j\in \argmin_{y\in\RR^d}\sum_{i\in I_j}p_i\lVert x_i-y\rVert^l.
$$
We could thus modify each centroïd $x_j$ by the optimal $y^*_j$ we found for $I_j$. Changing centroïds of the reduced set obtained by a local-search method decreases the value of $d_\ell\left(P,Q\right)$ hence the approximation ratio remains bounded. This algorithm is $m$-means algorithm but with a clever first guess. \clearpage

\begin{algorithm}[h]\caption{An algorithm for continuous scenario reduction}
    \label{improved ls}
    Run a local-search and extract $C_j$ for all $j$. \\
    With a method of your choice, find for each $C_j$: $$y_j\in\argmin_{y\in\RR^d}\sum_{i\in C_j}p_i\lVert x_i-y\rVert^l$$
    \\ Let $\{C_j\}\in\mathfrak{P}\left(I,m\right)$ be any partition whose sets $C_j, j\in J$, contain all atoms of supp$\left(P\right)$ that are closest to $y_j$ (ties may be broken arbitrarily). \\
    Repeat Step 2 and 3 until there is no more change.
\end{algorithm}

\section{$\quad$ Efficient implementation and results}\label{fast}
In this section, the goal is to compare different methods based on two criterion: time, value of $d_\ell\left(P,Q\right)$. The aim of this section is to determine which method is the most effective if an user does not provide specific instructions. We will explore many techniques. 
\subsection{Dupačová et al' algorithm, forward and backward methods}
\subsubsection{Outputs}
The first comparison to make is between the forward and backward methods of Dupačová et al' algorithm. We will use the euclidian norm and $\ell=2$. The efficient method proposed for the forward algorithm looks way more interesting at first sight as computing $\min_{k\in R}\lVert x_i-x_k\rVert^l$ is cheap from one step to another. I created a random two-dimensional data-set where each point is a sample of: $$
\begin{pmatrix}
    \mathcal{N}\left(10,2\right) \\ \Gamma\left(2,2\right)
\end{pmatrix}
$$
The probability of each sample $x_i$ is $\frac{1}{n}$. Let us take a look at the output we obtain when using both methods when we reduce the initial set to 10 atoms: 
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/dupacova backward.pdf}
        \caption{Output obtained with the backward algorithm}
        \label{result back}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/dupacova forward.pdf}
        \caption{Output obtained with the forward algorithm}
        \label{result forw}
    \end{subfigure}
    \caption{Output visualization: forward and backward Dupačová et al' algorithm}
\end{figure}
Obviously outputs are not the same. Circles represent atoms and crosses represent atoms that are selected by an algorithm in the reduced distribution and a color represents a cluster. For the simulations to come, we would rather use a bigger data-set in order to be more general. Every dimension should have a comparable data-range so that the distance between atoms is not led by a single or two dimensions. We will take samples of three normal distribution, two gamma and one uniform:
$$
\begin{pmatrix}
    \mathcal{N}\left(0,2\right) & \mathcal{N}\left(5,2\right) & \Gamma\left(0,2\right) & \Gamma\left(1,2\right) &\Gamma\left(2,2\right) & \mathcal{U}\left(0,10\right)
\end{pmatrix}^T.
$$

\subsubsection{Comparison}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/efficiency milp.pdf}
        \caption{$d\left(P,Q\right)^2$ comparison between both algorithms}
        \label{dist for bac}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/run time milp.pdf}
        \caption{Run time comparison}
        \label{time for bac}
    \end{subfigure}
    \caption{Efficiency and calculation time comparison: forward and backward Dupačová et al' algorithm}
    \label{comparison dup}
\end{figure}

The horizontal axis of \Cref{comparison dup} represents $m$, the number of atoms in the reduced set. \Cref{dist for bac} shows that the efficiency in term of Wasserstein distance is similar and comparable for a reduced set obtained via forward or backward algorithm but still the forward method gets a slight lead. The main difference lies in CPU time, the efficient manner we found for the forward algorithm shows its advantages in \Cref{time for bac}, always below 0.05s whereas the backward algorithm can take up to one second and takes more time than the exact method. Forward algorithm has to be preferred when you're free to choose even when $m\approx n$. We solve the LP reformulation with Gurobi and it is very efficient, it is comparable to the forward method when $n,m$ are small. \\
We have seen that backward method is not interesting in terms of CPU time. We will now compare forward method and MILP formulation for bigger distributions, it is $n\in\left[100,1000\right]$. Our goal is to understand the behaviours of both algorithms when $n$ increases because \Cref{time for bac} may be irrelevant as Gurobi is very efficient with small-sized problems. Let us say we want to reduce a distribution with $n$ atoms to a distribution with $\frac{n}{4}$ atoms.
\clearpage

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/milp efficiency n4.pdf}
        \caption{$d_2\left(P,R\right)^2$ comparison between both algorithms}
        \label{n4 effi}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/run time comparison n4.pdf}
        \caption{Run time comparison}
        \label{n4 time}
    \end{subfigure}
    \caption{With a bigger $n$}
    \label{comparison n4}
\end{figure}

\Cref{complexity} tells us that forward method has a complexity of $\mathcal{O}\left(mn^2\right)$ what is meant to be far less than solving a LP. Gurobi may use heuristics or other sophisticated method to have this small run time gap. Without Gurobi the gap would be way bigger. Still there exists a huge gap in CPU time we can not neglect especially when $n$ grows. Behaviours in efficiency are not affected by this change of scale for $m$ and $n$.
\subsection{Local search}
A run of local-search is not assured to end in polynomial time. Thus it can be somehow long to compute with a random first guest of size $m$. The first comparison we have to make is between best-fit and first-fit method.
\subsubsection{Best-fit or first-fit}
Like Dupačová et al' algorithm, local-search will give you an approximate of $D_\ell\left(P,m\right)$ but local-search methods are meant to be more satisfying as they provide guarantees, see \ref{guarant}. First, we want to understand how the local search methods behave in comparison to the greedy algorithm and especially the forward method. In this part, the first guess $R$ of size $m$ is the $m$-first elements of the data-set, it is a random guess.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/milp local search efficiency.pdf}
        \caption{Efficiency comparison, local search and forward Dupačová}
        \label{value loc s}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/run time local search.pdf}
        \caption{Calculation time comparison, local search methods and forward Dupačová}
        \label{time loc s}
    \end{subfigure}
    \caption{Efficiency and calculation time comparison: forward Dupačová and local search methods}
    \label{comparison loc}
\end{figure}

Run time grows linearly with $m$ with $n$ fixed for forward Dupačová et al' algorithm. For a run of local-search, as we do not know how many time we will enter the while loop for both local search methods, computation time looks messier for both method. Run time of the first-fit method tends to be messier compared to best-fit method, variability from $m$ to $m+1$ is more evident for the first-fit method. It aligns with the fact that best-fit is more stable as it compares all swap and chooses the best while first-fit swaps as soon as it finds an improvement \emph{even though it may be a very small one}, see \ref{step}. In terms of efficiency, both local-search methods give a better approximation than the one provided by forward Dupačová et al' algorithm. In the following figure, we represent a ratio $r_m$ for $n=100$ and $m\in [\![10;90]\!]$. We define this ratio $r_m$ for a certain $m$ as: 
$$
r_m = \frac{d_\ell\left(P,Q\right)}{D_\ell\left(P,m\right)}.
$$
We know $D_\ell\left(P,m\right)$ thanks to the MILP formulation and $Q$ is generated with local-search or any other algorithm.

\begin{figure}[ht]
    \centering
    \includegraphics[width=0.45\textwidth]{plots/ratio with milp.pdf}
    \caption{Approximation ratio for $n=100$}
\end{figure}
    
\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/run time 200.pdf}
        \caption{Run time for $n=200$}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=\textwidth]{plots/ratio 200.pdf}
        \caption{Approximation ratio for $n=200$}
    \end{subfigure}
    \caption{Comparison for $n=200$ and variable $m$}
    \label{improvement ratio }
\end{figure}

For both $n=100$ and $n=200$, local-search gives a better approximation to $D_l\left(P,m\right)$ to the detriment of CPU time. Both best-fit and first-fit can be used even though the tendency is that first-fit has more run time variation from $m$ to $m+1$ when best-fit tends to be more stable. Best payouts for local-search are seen when you want to reduce $n$ to $m\leq\frac{n}{2}$. When $m\approx n$, there may still be a gain but thin. 

\subsubsection{Modified versions of local-search}
Local-search can be very excruciating but still we can play with different variables. E.g. not allowing small swaps for first-fit, use a clever set of starters. We will now present and compare variations of the local-search method with the aim of reducing run time without damaging efficiency. 

\paragraph{\textbf{Clever starters as a very first guess.}}
We decide to use the reduced set of size $m$ that we obtain with the forward Dupačová et al' algorithm, the backward algorithm and the set of of atoms that are the closest to the centroïds computed with a run of $m$-means on the data-set. The idea of this section is to compare efficiency and computation time between these methods and a random first guess. We will still compute the data-set as samples of the 6D data-set we have used so far. We can expect to reduce calculation time with an interesting starter $R$ of cardinal $m$ because it should lead to fewer changes in the local-search hence a lower run time. As order is important in first-fit, we will also use the reversed set obtained via forward Dupačová et al' algorithm. Using the reversed sequence of Dupačová et al' algorithm can be odd as the first atom of the reversed sequence will never be swapped but second, third, fourth are more likely to be changed as they weren't chosen at first, it is an idea we must consider. Inversely, the first atom chosen was the most centered so it gave a real direction to the choice that came later so changing this one would change a lot of things. Both ideas have their arguments that is why we must investigate. Note that best-fit for the original sequence or the reversed sequence must give the same output.

\begin{figure}[!h]
    \centering
    \includegraphics[width=0.5\linewidth]{plots/good methods.pdf}
    \caption{Run time comparison with clever starters}
    \label{good methods}
\end{figure}

We recall in \Cref{good methods} methods that are the most efficient in terms of run time. \Cref{good methods} is satisfying because all methods we proposed reduce run time in general and sometimes by more than a factor 5. The run time of any method that uses either $m$-means or Dupačová et al' algorithm includes the run time of the respective algorithm. Most interesting methods are: best-fit with $m$-means startersn, first-fit and best-fit with Dupačová starters. The main difference between $m$-means and Dupačová starters is the fact that $m$-means consider all $m$ starters at once whereas Dupačová finds its starters one at a time. We did not find any conclusion to make on the reversed or not sequence of Dupačová starters but still run time varies when using the reversed sequence or not as expected. In Appendix \ref{all methods}, you can find an overwhelming figure with all methods we have tried, as well as the comparison in efficiency in terms of Wasserstein distance. Wasserstein distances are similar for all methods what is relieving. 


\paragraph{\textbf{First-fit with a minimum step.}}\label{step}
First-fit method lacks of robustness. To reduce run time an idea is to allow changes that sufficiently reduce the cost function. The swap will be accepted if and only if: $$
d_\ell\left(P,Q_{i+1}\right) \leq d_\ell\left(P,Q_i\right) - \epsilon.
$$
Where $Q_i$ was the distribution before iteration and $Q_{i+1}$ a distribution we are testing with one swap compared to $Q_i$. The idea of this part is to find an interesting $\epsilon$ to allow efficiency in a first-fit method. To be general and not to rely on a specific problem, we are testing $\epsilon$ as: $$\epsilon\left(p\right)=pd_\ell\left(P,D\right),$$ where $p$ is a parameter that has to be defined and $d_\ell\left(P,D\right)$ the distance between the original distribution $P$ and $D$ the distribution given by the forward Dupačová et al' algorithm. We will compare run time and efficiency in terms of $p$:

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{plots/efficiency p.pdf}
        \caption{Efficiency}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{plots/run time p.pdf}
        \caption{Run time}
    \end{subfigure}
    \caption{Comparison, $n=200$, $m=50$}
    \label{p}
\end{figure}

Both figures are saying that we must study this algorithm in the range $p\in\left]0,0.015\right]$, where it grants a lower run time and a better output than Dupačová.

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{plots/efficiency small scale p.pdf}
        \caption{Efficiency}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.45\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{plots/RUN TIME p small scale.pdf}
        \caption{Run time}
    \end{subfigure}
    \caption{Comparison, $n=200$, $m=50$, scale of interest}
    \label{p small scale}
\end{figure}

An interesting remark would be to say that with $p\approx 0.001$, this method even gives a better approximate to $D_\ell\left(P,m\right)$ than the original first-fit meaning $p=0$. Still this remark exists for a particular distribution here maybe we can not assume that statement in general. When $p$ increases, run time reduces as well as efficiency. 
\begin{remark}
    Through the parameter $p$ this method provides a \textbf{continuous transformation} of the space between the approximation made by Dupačová et al' algorithm and local-search. 
\end{remark}

\paragraph{\textbf{Using randomization in first-fit.}}
The first-fit method relies on a linear exploration of the current solution until you find an improvement. Exploring the current solution always with the same method can lead to a lot of useless calculation, e.g. if you know that you only have to swap either the last atom or the one before but you explore linearly the current solution. The idea is to randomize the exploration order by introducing a shuffle so that the exploration order is never the same. In order to generate these permutations, we use the algorithm of Fisher-Yates, this algorithm generates a permutation of $n$ elements in a linear run time, all permutations being equally probable.
\begin{algorithm}\caption{Fisher-Yates algorithm}\label{fisher yat}
    \For{$i$ from $n-1$ down to $1$}{
    $j\gets k$, $k$ randomly chosen in $ [\![0,i]\!]$  \\
    Exchange the value in $i^{\text{th}}$ position with the one in $j^{\text{th}}$.}
\end{algorithm}
For this simulation we use $n=100$, $m$ as the $x$-axis and  starters. Note that we run 25 times the random first-fit method to get the plot "mean random first-fit"
\begin{figure}[ht]
    \centering
    \includegraphics[width=0.5\textwidth]{plots/random first fit.pdf}
    \caption{Random first-fit results}
    \label{random ff}
\end{figure}
Using this randomized order helps us to gain stability in run time. Even the worst case of this randomized method is generally better or at least comparable to the deterministic first-fit. There is a clear gain. If the randomization procedure was $\mathcal{O}\left(1\right)$ the gain would have been even better because this procedure adds one more step in the algorithm for each time you want to swap. The gain in run time seems to be huge for a small $m$, when $m\approx n$, this procedure does not seem to be a good idea.

\subsection{Fusion of local search and $m$-means}
In this part we want to compare \Cref{improved ls}  with a generalized $m$-means for an approximation of $C_l$ as proposed in \Cref{k m} and \Cref{Local search}. For the sake of run time and thanks to the above analysis, we will use the best-fit method with Dupačová starters whenever we need to run a local-search. We will only optimize on the centroïds once meaning that we compute the reduced set with a run of local-search and then we reoptimize on the centers, $y_j^*=\text{mean}\left(I_j\right)$ because we work with the euclidian distance and $\ell=2$.


\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{plots/effiency cl.pdf}
        \caption{Efficiency}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{plots/efficiency ratio cl.pdf}
        \caption{Improvement ratio}
    \end{subfigure}
    \caption{Efficiency comparison, $n=150$}
    \label{cl }
\end{figure}


The $m$-means algorithm tends to be a little better than local-search in this simulation even though its approximation ratio is not bounded. Nevertheless, it remains understandable to have a closer distribution with $m$-means because it can decide to use atoms that are not included in $\text{supp}\left(P\right)$. Combining the idea of $m$-means with local-search gives in general a slightly better value and most important it presents more stability and by stability I mean that the efficiency curve is smoother.

\section{$\quad$ The multiproduct assembly problem}\label{section mpa}
In this section, we will apply scenario reduction to the \emph{multiproduct assembly problem} with finitely many demand scenarios. This assumption can be allowed as the manufacturer may use $k$ past demands with a probability $\frac{1}{k}$ to model the future demand.
\subsection{Problem}
\cite[Chapter 1.3]{shapiro_lectures_2009} gives a clear presentation of the problem that I am writing, I will be adding notes when necessary. A manufacturer produces $n$ \emph{products}. Each product is composed by \emph{parts}. There are $m$ parts. The product $i$ requires $a_{ij}$ units of part $j$, then $a_{ij}\in\mathbb{N}$. Demands are modeled as a random vector $D=\begin{pmatrix} D_1 & \hdots & D_n \end{pmatrix}$ note that the distribution of $D$ is known. Before the demand is known the manufacturer may preorder parts at a unit cost of $c_j$ for a unit of part $j$. After the demand $D$ is observed, the manufacturer has to decide which portion of the demand is to be satisfied with her or his parts inventory. It costs additionally $l_i$ to satisfy a unit of demand for product $i$ and the unit selling price of this product is $q_i$. The unused parts are saved and are given a value $s_j < c_j$, saving a parts is not a loss but the value given to this saving can not exceed the purchase price. Now suppose that the numbers of parts ordered during the first-stage are $x_j, j=1,\hdots,m$. We want to know how to produce. Let $z_i,i=1,\hdots,n$ be the number of units of product $i$ produced and $y_j$ the numbers of part $j$ left in the inventory. After the demand $D$ is known, production is decided in order to minimize losses:
\begin{align*}
    \min_{y,z} &\sum_{i=1}^n\left(l_i-q_i\right)z_i-\sum_{j=1}^ms_jy_j \\
     \text{s.t. } &y_j=x_j-\sum_{i=1}^na_{ij}z_i, \: j=1,\hdots,m \\
     & 0 \leq z_i\leq d_i, \: i=1,\hdots,n, \: y_j\geq0, \: j=1,\hdots,m.
\end{align*}
We can rewrite it:
\begin{align*}
    \min_{y,z} & \left(l-q\right)^Tz-s^Ty \\
     \text{s.t. } &y=x-A^Tz, \\
     & d \geq z\geq 0, \:y\geq0.
\end{align*}
One can convince itself that the recourse is fixed. The value of this optimization problem depends on $x$ and on a demand $d$. We write $Q\left(x,d\right)$ the optimal value of this problem. The two-stage linear problem associated to the multiproduct assembly problem is:
$$\min_{x\geq0}c^Tx+\mathbb{E}_DQ\left(x,D\right).$$
\subsection{Data-set}
Data-set on this kind of problems do not exist or if they do, it did not find any. The objective of this section is to build a data-set. Let us put ourselves in a carpenter's shoes.
\subsubsection{Products}
\begin{enumerate}
    \item Table
    \item Chairs
    \item Lawn table
    \item Lawn chairs
    \item Bed
    \item Desk
    \item Wardrobe
    \item Shelf
    \item Bookcase
    \item Shoe cabinet
\end{enumerate}
\subsubsection{Components}
\small
\begin{longtable}{| l | c | c | c | c | c | c | c | c | c | c |}
\hline
\textbf{Product} & \textbf{Tabletop} & \textbf{Legs} & \textbf{Screws} & \textbf{Seat} & \textbf{Backrest} & \textbf{Coating} & \textbf{Frame} & \textbf{Headboard} & \textbf{Footboard } & \textbf{Slats } \\ 
\hline
Table & 1 & 4 & 16 & - & - & - & - & - & - & - \\ 
\hline
Chairs & - & 4 & 8 & 1 & 1 & - & - & - & - & - \\ 
\hline
Lawn Table & 1 & 4 & 16 & - & - & 1 & - & - & - & - \\ 
\hline
Lawn Chairs & - & 4 & 8 & 1 & 1 & 1 & - & - & - & - \\ 
\hline
Bed & - & - & 16 & - & - & - & 1 & 1 & 1 & 1 \\ 
\hline
Desk & - & 4 & 16 & - & - & - & - & - & - & - \\ 
\hline
Wardrobe & - & - & 32 & - & - & - & 1 & - & - & - \\ 
\hline
Shelf & - & - & 16 & - & - & - & - & - & - & - \\ 
\hline
Bookcase & - & - & 16 & - & - & - & - & - & - & - \\ 
\hline
Shoe Cabinet & - & - & 16 & - & - & - & - & - & - & - \\ 
\hline
\end{longtable}

\small
\begin{longtable}{| l | c | c | c | c | c | c | c | c | c |}
\hline
\textbf{Product} & \textbf{Box Spring} & \textbf{Desktop} & \textbf{Pedestals} & \textbf{Drawers} & \textbf{Doors} & \textbf{Shelves} & \textbf{Rod} & \textbf{Side Panels} & \textbf{Back Panel} \\
\hline
Table & - & - & - & - & - & - & - & - & - \\
\hline
Chairs & - & - & - & - & - & - & - & - & - \\
\hline
Lawn Table & - & - & - & - & - & - & - & - & - \\
\hline
Lawn Chairs & - & - & - & - & - & - & - & - & - \\
\hline
Bed & 1 & - & - & - & - & - & - & - & - \\
\hline
Desk & - & 1 & 2 & 1 & - & - & - & - & - \\
\hline
Wardrobe & - & - & - & - & 2 & 5 & 1 & 2 & 1 \\
\hline
Shelf & - & - & - & - & - & 4 & - & 2 & 1 \\
\hline
Bookcase & - & - & - & - & - & 5 & - & 2 & 1 \\
\hline
Shoe Cabinet & - & - & - & - & 2 & 4 & - & 2 & 1 \\
\hline
\end{longtable}
It defines the matrix $A$.

\subsubsection{Rewards and costs}

We present costs and rewards fot both products and subsassemblies.


\begin{table}[ht]
    \centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
    $q$  &  300 & 100 & 350 & 120 & 500 & 200 & 300 & 70 & 50 & 50 \\
    \hline
    $l$  & 0 &0&0&0&0&0&0&0&0&0 \\
    \hline
\end{tabular}
\end{table}



\begin{table}[ht]
    \centering
\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|c|}
\hline
    $s$  &  20 & 1 & 0.1 & 1 & 1 & 2 & 10 & 4 & 4 & 6 &6&2&2&4&6&2&2&4&4\\
    \hline
    $c$  & 100 & 5 &0.5& 5 & 5& 10& 50 & 20 & 20 & 30 & 30 &10& 10 & 20&30&10&10&20&20 \\
    \hline
\end{tabular}
\end{table}
\subsubsection{Demands}
We will create $K$ past demands, each with a probability of $\frac{1}{K}$. Demands will be derived from a uniform distribution. Each demand will be generated as:
$$
D = \begin{pmatrix} \mathcal{B}\left(40,0.5\right) \\ 4d_1 + \mathcal{U}_\mathbb{N}\left(0,25\right)
  \\ \mathcal{B}\left(40,0.5\right) \\ 4d_3 + \mathcal{U}_\mathbb{N}\left(0,25\right)
  \\ \mathcal{B}\left(40,0.5\right) \\ \mathcal{B}\left(40,0.5\right) \\ \mathcal{B}\left(40,0.5\right) \\ \mathcal{B}\left(40,0.5\right) \\ \mathcal{B}\left(40,0.5\right) \\ \mathcal{B}\left(40,0.5\right) \\ \end{pmatrix}.
$$
One can note that a vector of random uniform demands is not appropriate. In a few words, it is not appropriate because when reoptimizing the weights of the reduced set, some are more influent than other even though the demand is uniform. Results obtained without reoptimizing weights (with uniform probabilities) are often closer to the original optimal value. Thanks to \ref{stability}, solving these reduced problems are supposed to give a solution that is close the original optimum.

\subsection{Results}
We have presented everything we need to solve this multiproduct assembly problem. We must now solve it. In our case, there are finitely many demand scenarios. The probability $p_k$ of observing the demand $d_k$ is $\frac{1}{K}$ for each scenario. Hence, we replace $\mathbb{E}_D\left(Q,x,D\right)$ by $\sum_{k=1}^Kp_kQ\left(x,d_k\right)$. We will therefore reduce the number of scenarios in order to reduce the length of the sum, the number of constraints and so run time. We now have to solve:

\begin{equation}\label{LP}
\begin{aligned}
    \min\; &c^Tx + \sum_{k=1}^{K} p_k \left[ (l - q)^T z^k - s^T y^k \right] \\
    \text{s.t. } & y^k = x - A^T z^k, \quad k = 1, \ldots, K \\
    & 0 \leq z^k \leq d^k, \quad y^k \geq 0, \quad k = 1, \ldots, K \\
    & x \geq 0
\end{aligned}
\end{equation}

In \ref{fast}, we have presented different ways of considering some scenario reduction algorithms in order to reduce run time. Now we will use these routines when we need them - efficient forward Dupačová, random first-fit, best-fit with Dupačová starters -  in order to reduce the size of this problem by reducing the number of scenarios of $D$ and so reduce run time of this LP. With $n=250$ scenarios, we will reduce the number to $m\in\{25,\hdots,125\}$ scenarios and plot the ratio: $$\lvert\frac{v(P)-v(Q_m)}{v(P)}\rvert,$$ where $Q_m$ is a reduced distribution with $m$ scenarios generated via either a local-search algorithm or forward Dupačová algorithm. For the sake of consistency, we will also plot $d_\ell\left(P,Q_m\right)^\ell$ in order to visualize that a closer distribution will generally lead to closer optimal values. We will also see how scenario reduction impact run time. 

\begin{figure}[ht]
    \centering
    \subfloat[Wasserstein distance]{
        \includegraphics[width=0.47\textwidth]{plots/distance two stage.pdf} 
    }
    \hfill
    \subfloat[Ratio]{
        \includegraphics[width=0.47\textwidth]{plots/ratio 250 two stage.pdf} 
    }
    \hfill
    \subfloat[Run time]{
        \includegraphics[width=0.47\textwidth]{plots/run time two stage.pdf} 
    }
    \caption{Figures for two-stage stochastic programming with $n=250$.}
\end{figure}

These figures present: Wasserstein distance between the original distribution and reduced ones (either via local-search or greedy algorithm), the optimal value ratio and run time. Both local-search and greedy algorithm give comparable ratios (less than 2\% when reducing the number of scenarios by a factor 10) even though local-search tends to give better approximation as it gives a distribution that is generally closer to the original one. Obviously when $m$ grows, distribution generated are closer to the original one, hence, it gives a smaller ratio in general. This observation is comforting with the Lipschitz nature of the problem: $$\lvert v\left(P\right)-v\left(Q\right)\rvert \leq C d_\ell\left(P,Q\right)^\ell.$$ A distribution that is closer to the original one will generally lead to a closer optimal value. There can be a gap sometimes. Run time grows linearly with $m$ because Gurobi is very efficient for small scales LP. Solving the LP \ref{LP} with $n=2000$ takes about 29 seconds, when the reduced LP with $m=200$ is completed in about 2 seconds. Cutting the number of scenarios by a factor 10 is clearly not threatening here and reduces run time. Generating this new scenario set can be excruciating but still it can be very interesting in some cases e.g. if you dare optimizing on cost vector in a multiproduct assembly problem with a certain distribution of demands. You can reduce once demands and then use the reduced distribution to save run time when solving again and again the LP.
\section{$\quad$ A new cost}
\section{$\quad$ Conclusion}
\clearpage
\bibliography{biblio}
\bibliographystyle{alpha}
\clearpage


\begin{appendices}
    
\section{$\quad$ Fortet-Mourier is a distance}\label{appendix metric}

\section{$\quad$ Definiteness of Wasserstein distance}\label{definiteness}
\noindent Let $\ell\geq 1$, $P=\sum_{i\in I}\delta_{x_i}p_i$ and $Q=\sum_{j\in J}\delta_{y_j}q_j$ such that $d_\ell\left(P,Q\right)=0$. We prove that $d_\ell\left(P,Q\right)=0\implies \text{supp}\left(P\right)=\text{supp}\left(Q\right)$. By contradiction, let us assume that $\text{supp}\left(P\right)\ne\text{supp}\left(Q\right)$. Then there exists $x_{i_0}\in \text{supp}\left(P\right)$ such that $x_{i_0}\neq y,$ for all $ y\in\text{supp}\left(Q\right)$ (if not, then a $y_{i_0}$ exists in $\text{supp}\left(Q\right)$ and $y_{i_0}\notin \text{supp}\left(P\right)$ as the support are different). As $d_l\left(P,Q\right)=0$ we have that:$$
    \exists\pi\in\RR^{nm}_+ : \sum_{i\in I}\pi_{ij}=q_j, \sum_{j\in J}\pi_{ij}=p_i, \forall i,j \text{ and } \sum_{j\in J}\sum_{i\in I}\pi_{ij}\lVert x_i-y_j\rVert^\ell=0.
    $$
Take a look at the last equation, each term of the sum is positive, then it follows that:
    \begin{equation}\label{useful}
    \forall i,j \in I\times J, \: \pi_{ij}\lVert x_i-y_j\rVert^\ell=0.
    \end{equation}
    We can forget the $\ell^{th}$ power:
    $$
     \forall i,j \in I\times J, \: \pi_{ij}\lVert x_i-y_j\rVert=0.
    $$
    But we said that there exists $x_{i_0}\in\text{supp}\left(P\right)\setminus\text{supp}\left(Q\right)$. It follows that $\forall j\in J, \pi_{i_0j}=0$ what's impossible because of the constraints we had, $0=\sum_{j\in J}\pi_{i_0j}=p_{i_0}\neq0$, what is absurd. Then $$d_\ell\left(P,Q\right)=0\implies \text{supp}\left(P\right)=\text{supp}\left(Q\right).$$
    W.l.o.g. we can renumber the atom and then say $p_i$ and $q_i$ are associated to the same atom. We use \Cref{useful} and we have that :
    $$
    \forall k\in I, k\neq i, \pi_{ki}=0, \pi_{ik}=0.
    $$
    Then the constraints on $\pi$'s sum on rows and columns give  : $$ 
    \sum_{k\in I}\pi_{ki}=\pi_{ii}=q_i \text{ and } \sum_{k\in J}\pi_{ik}=\pi_{ii}=p_i.
    $$
    We've proven the definiteness of $d_l$. Because supports and weights are the same for both distribution : $P=Q$.

\section{$\quad$ Invariant under translation }\label{appendix restrict}
\noindent Let $x_i,y_j\in\RR^d$, $P=\sum_{i\in I}\delta_{x_i}p_i$ with $n$ atoms, $m<n$, $Q=\sum_{j\in J}\delta_{y_j}q_j$ with $m$ atoms, $\ell\geq1$, $\lambda\in\RR^*$, and $a\in\RR^d$. We write $P_a=\sum_{i\in I}\delta_{x_i+a}p_i$. We have that:
\begin{align*}
d_\ell(P_a,Q)&=\left(\min_{\pi\in\mathbb{R_+^{|I|\times|J|}}}\left\{ 
\sum_{i\in I}\sum_{j\in J}\pi_{ij}\lVert \left(x_i+a\right)-y_j\rVert^\ell \: \text{ : } \:  \begin{aligned}
& \sum_{j\in J}\pi_{ij}=p_i \: \forall i\in I \\
& \sum_{i\in I}\pi_{ij}=q_j \: \forall j\in J
\end{aligned}\right\}\right)^{1/\ell}
\\ &= \left(\min_{\pi\in\mathbb{R_+^{|I|\times|J|}}}\left\{ 
\sum_{i\in I}\sum_{j\in J}\pi_{ij}\lVert x_i-\left(y_j-a\right)\rVert^\ell \: \text{ : } \:  \begin{aligned}
& \sum_{j\in J}\pi_{ij}=p_i \: \forall i\in I \\
& \sum_{i\in I}\pi_{ij}=q_j \: \forall j\in J
\end{aligned}\right\}\right)^{1/\ell}
\\ &=d_\ell\left(P,Q_{-a}\right).
\end{align*}
From this equality, it follows easily that:
$$
C_\ell\left(P_a,m\right)=C_\ell\left(P,m\right)
$$
For $D_\ell$, $\text{supp}\left(P_a\right)=\text{supp}\left(P\right)+a$, thus we can write for $Q$ such that $\text{supp}\left(Q\right)\subset \text{supp}\left(P\right)$, $\text{supp}\left(Q_a\right)\subset \text{supp}\left(P_a\right)$:
$$
d_\ell(P_a,Q_a)=d_\ell(P,Q).
$$
As we would have done for $C_\ell$:
$$
D_\ell\left(P_a,m\right)\leq d_\ell(P_a,Q_a) = d_\ell(P,Q).
$$
But the left-hand side does not depend on $Q$, we can minimize over $Q$ where the support of $Q$ is contained in the support of $P$. We can write:
$$
D_\ell\left(P_a,m\right)\leq D_\ell\left(P,m\right).
$$
Similarly we can prove the opposite and then it follows that:
$$
D_\ell\left(P_a,m\right)= D_\ell\left(P,m\right).
$$
\section{$\quad$ Positively homogenous}\label{appendix homogen}

\section{$\quad$ Local-search methods}\label{all methods}

\begin{figure}[ht]
    \centering
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{plots/run time methods.pdf}
        \caption{Run time}
    \end{subfigure}
    \hfill
    \begin{subfigure}[b]{0.495\textwidth}
        \centering
        \includegraphics[width=1.1\textwidth]{plots/value methods.pdf}
        \caption{Efficiency}
    \end{subfigure}
    \caption{Comparison of local-search variants}
    \label{methods}
\end{figure}

\end{appendices}
\end{document}